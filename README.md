# ğŸ¯ CÃ³mo funciona la funcionalidad Sampling en Model Context Protocol

Â¡Hola developer ğŸ‘‹ğŸ»! En este repositorio tienes un ejemplo de cÃ³mo funciona el Sampling en Model Context Protocol. Para que puedas entender perfectamente cÃ³mo puedes usarlo y quÃ© te aporta dentro de tus MCP Servers. Si quieres verlo en acciÃ³n, aquÃ­ tienes el vÃ­deo relacionado con este repo:

[![QuÃ© es el sampling en model context protocol](https://github.com/user-attachments/assets/1a8b25f6-8234-471d-8ea3-17c622ac7ce6)](https://youtu.be/7LARYKzChMQ)

## ğŸ¤” Â¿CÃ³mo funciona el Sampling en MCP?

El *sampling* en MCP es una caracterÃ­stica poderosa que permite que las tools/prompts de tus servidores MCP soliciten al cliente que genere contenido usando sus propios modelos LLM. En lugar de que el servidor MCP tenga que integrar directamente con proveedores de IA, delega esta responsabilidad al cliente.

### ğŸ”„ Flujo completo del Sampling

El proceso de sampling en MCP sigue este flujo:

1. **El cliente llama a una herramienta MCP** (ej. `summarize`) con parÃ¡metros especÃ­ficos.
2. **La herramienta utiliza `extra.sendRequest`** para enviar una solicitud de tipo `sampling/createMessage` al cliente.
3. **El cliente procesa la solicitud de sampling** usando sus modelos LLM disponibles.
4. **El cliente devuelve el contenido generado** a la herramienta MCP.
5. **La herramienta procesa la respuesta** y la devuelve al cliente original.

### â­ Ventajas del Sampling

- **Sin dependencias externas**: El servidor MCP no necesita API keys ni integraciones con proveedores de IA.
- **Flexibilidad del cliente**: El cliente puede elegir el modelo LLM mÃ¡s adecuado para cada tarea.
- **Eficiencia**: Evita llamadas redundantes a APIs externas.
- **Seguridad**: Las credenciales y configuraciones del LLM permanecen en el cliente.

### ğŸ’¡ Ejemplo prÃ¡ctico

```typescript
const result = await extra.sendRequest(
  {
    method: "sampling/createMessage",
    params: {
      messages: [
        {
          role: "user",
          content: {
            type: "text",
            text: `Please summarize the following text concisely:\n\n${text}`,
          },
        },
      ],
      maxTokens: 500,
      modelPreferences: {
        costPriority: 0.5,      // Balance costo y rendimiento
        intelligencePriority: 0.5, // Balance inteligencia y rendimiento  
        speedPriority: 0.5,     // Priorizar velocidad
      },
    },
  },
  CreateMessageResultSchema
);
```

### ğŸ”§ ImplementaciÃ³n correcta del Sampling

A diferencia de implementaciones incorrectas que podrÃ­an causar dependencias circulares, la implementaciÃ³n correcta utiliza el mÃ©todo `sampling/createMessage` a travÃ©s de `extra.sendRequest`. Es decir, no se debe intentar enviar un mensaje directamente desde la instancia del servidor, sino que debe enviarse desde la sesiÃ³n del cliente MCP. Para ello puedes usar el parÃ¡metro `extra` que recibe la tool.


```typescript
 async function (
      { text },
      extra: RequestHandlerExtra<any, any>
    ): Promise<CallToolResult> {
```


## ğŸš€ Uso

### ğŸ”§ Iniciar el servidor

```bash
npm install
npm run build
npm start
```

### ğŸ› ï¸ ConfiguraciÃ³n para Visual Studio Code

Este repositorio ya incluye la configuraciÃ³n necesaria para que puedas configurar este MCP Server en Visual Studio Code. La misma se encuentra en el archivo `.vscode/mcp.json`. Por lo que una vez arrancado el proyecto solo tienes que hacer clic en `Start` dentro del archivo `mcp.json` ğŸ¤“.


---

## ğŸ“º Â¡SuscrÃ­bete a mi canal de YouTube!

Si te ha resultado Ãºtil este contenido y quieres aprender mÃ¡s sobre Model Context Protocol, inteligencia artificial y desarrollo, Â¡no olvides suscribirte a mi canal de YouTube! AsÃ­ estarÃ¡s al tanto de nuevos tutoriales, ejemplos prÃ¡cticos y novedades del mundo tech.  
[ğŸ‘‰ SuscrÃ­bete aquÃ­](https://www.youtube.com/@returngis) y activa la campanita para no perderte nada.

---


Â¡Nos vemos ğŸ‘‹ğŸ»!
